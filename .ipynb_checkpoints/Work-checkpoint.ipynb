{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STT: 34\n",
    "\n",
    "**Member**\n",
    "\n",
    "18120374 Nguyễn Minh Hiếu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thư viện cần dùng thêm:\n",
    "   * **ntlk**: để schemming\n",
    "   \n",
    "   * **re** để làm chuẩn\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntlk.download()\n",
    "#Hiện bảng Chọn popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cần làm xử lí để đọc 2 file `check.txt` và `stopword.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lợi ích:\n",
    "* Nếu tìm được công thức thì công ty sản xuất có thể tự động hóa quá trình rút trích ý kiến thích / không thích từ các đoạn đánh giá của người dùng\n",
    "* Giúp công ty đánh giá được hiệu quả công việc. (Ở đây là AMAZON có thể kiểm tra sản phẩm được vận chuyển có đúng ý muốn khách hàng hay không)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Thu thập dữ liệu\n",
    "Đưa một link sản phẩm bất kì và kiểm tra phần đánh giá của khách hàng.\n",
    "* Đánh giá sẽ phân loại: ***postive* : 1**  và ***critical* : 0**\n",
    "* Mỗi trang gồm 10 đánh giá, ta duyệt 250 trang cho mỗi loại đánh giá. Vậy tổng cộng 10x270x2 = 5400 đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com/NETGEAR-R6700-Nighthawk-Gigabit-Ethernet/dp/B00R2AZLD2/ref=gbps_img_m-9_475e_daf1a9de?smid=ATVPDKIKX0DER&pf_rd_p=5d86def2-ec10-4364-9008-8fbccf30475e&pf_rd_s=merchandised-search-9&pf_rd_t=101&pf_rd_i=15529609011&pf_rd_m=ATVPDKIKX0DER&pf_rd_r=B32H66M6KMBDBB378HQH&th=1'\n",
    "browser = webdriver.Chrome(executable_path='./chromedriver.exe') \n",
    "browser.maximize_window()\n",
    "browser.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Phrases` Gồm các đánh giá\n",
    "\n",
    "`status` loại đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "status = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click See all review\n",
    "review_page = browser.find_element_by_link_text('See all reviews').click();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose Verified purchase only\n",
    "reviewer_types = browser.find_elements_by_css_selector('#reviewer-type-dropdown option')\n",
    "for reviewer_type in reviewer_types:\n",
    "    if \"Verified purchase only\" in reviewer_type.text:\n",
    "        reviewer_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive first\n",
    "comment_types = browser.find_elements_by_css_selector('#star-count-dropdown option')\n",
    "for comment_type in comment_types:\n",
    "    if \"All positive\" in comment_type.text:\n",
    "        comment_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page content\n",
    "for i in range(0,3):\n",
    "    time.sleep(3)\n",
    "    html_text = browser.page_source\n",
    "    tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    reviews = tree.find_all('div',{'data-hook':'review'})\n",
    "    for review in reviews:\n",
    "        content = review.find('a', {'data-hook': 'review-title'})\n",
    "        phrases.append(content.text.strip())\n",
    "        status.append(1)\n",
    "    browser.find_element_by_class_name('a-last').click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then Critical\n",
    "comment_types = browser.find_elements_by_css_selector('#star-count-dropdown option')\n",
    "for comment_type in comment_types:\n",
    "    if \"All critical\" in comment_type.text:\n",
    "        comment_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page content\n",
    "for i in range(0,3):\n",
    "    time.sleep(3)\n",
    "    html_text = browser.page_source\n",
    "    tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    reviews = tree.find_all('div',{'data-hook':'review'})\n",
    "    for review in reviews:\n",
    "        content = review.find('a', {'data-hook': 'review-title'})\n",
    "        phrases.append(content.text.strip())\n",
    "        status.append(0)\n",
    "    browser.find_element_by_class_name('a-last').click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data = pd.DataFrame(\n",
    "    {'Para': phrases,\n",
    "     'Stt': status\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kiểm tra data**\n",
    "\n",
    "Lưu data vào file \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data.to_csv('data.csv',index = False,encoding='utf-16',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sử dụng file data.csv có sẵn\n",
    "* cột Para sẽ chứa các đánh giá\n",
    "* cột status gồm loại đánh giá tương ứng (1: positive - 0: critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Para</th>\n",
       "      <th>Stt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>Great Product!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Para  Stt\n",
       "2458  Great Product!    1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv',encoding='utf-16',sep = '\\t')\n",
    "#data.head()\n",
    "data[data.index == 2458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu trùng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loại bỏ trùng lặp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5074, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra thiếu dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Para    0\n",
       "Stt     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra độ cân bằng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48817500985415846"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Stt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sr = data[\"Stt\"] # sr là viết tắt của series\n",
    "X_df = data.drop(\"Stt\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df, val_X_df, train_y_sr, val_y_sr = train_test_split(X_df, y_sr, test_size=0.3, \n",
    "                                                              stratify=y_sr, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3551, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3551,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_sr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 2 tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cần thực hiện các bước: loại phần thừa, áp dụng *stemming* \n",
    "* Stemming: đưa các từ có hình thức tương đồng nhau về từ gốc (Ở đây ta chưa bàn về ngữ cảnh).\n",
    "* Phần thừa như dấu câu, icon.\n",
    "\n",
    "Vì ta sử dụng tiếng Anh nên các từ có dấu (khác a-z A-Z) cũng sẽ bị loại bỏ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import  StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steeming English only\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# đọc file check.txt\n",
    "# đọc file stopword.txt\n",
    "\n",
    "rồi xử lý trong hàm dưới"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm `Recontruct_Sentence` để xử lý câu:\n",
    "   * Làm chuẩn câu\n",
    "   * Tối giản từ\n",
    "   * ghép các từ lại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recontruct_Sentence(sentence):\n",
    "    sentence = re.sub('[^A-Za-z]',' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(englishStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `trans_Sentence` sẽ chuyển DataFrame đầu vào thành DataFrame tối giản với phương thức *transform*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pipeline\n",
    "class trans_Sentence(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X_df, y=None):\n",
    "        return self\n",
    "    def transform(self, X_df, y=None):\n",
    "        # YOUR CODE HERE\n",
    "        optimize= []\n",
    "        self = X_df.copy()\n",
    "        sentence_s = self.Para\n",
    "        for sentence in sentence_s:\n",
    "            optimize.append(Recontruct_Sentence(sentence))\n",
    "        self.drop(['Para'],axis = 1)\n",
    "        self['Para'] = optimize\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer() tham khảo ở [link](https://thorpham.github.io/blog/2018/04/14/setmentation/) và [doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "Class `toarray_Tidf` gồm các tham số đầu vào dành cho TfidfVectorizer().\n",
    "* fit: dùng `TfidfVectorizer()` fit tập dữ liệu đầu vào \n",
    "* transform dùng `TfidfVectorizer()` biến dữ liệu đầu vào thành mảng các vector dựa trên fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toarray_Tidf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,max_features = None, ngram_range = None, stop_words = None):\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.stop_words = stop_words \n",
    "        vect = TfidfVectorizer(max_features =  max_features, ngram_range =  ngram_range, stop_words =  self.stop_words)\n",
    "        self.vect = vect\n",
    "    def fit(self, X_df, y=None):\n",
    "        self.vect.fit(X_df.Para)\n",
    "        return self\n",
    "    def transform(self, X_df, y=None):\n",
    "        # YOUR CODE HERE\n",
    "        return self.vect.transform(X_df.Para).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xây dựng mạng Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans_sentence', trans_Sentence()),\n",
       "                ('toarray_tidf',\n",
       "                 toarray_Tidf(ngram_range=(1, 1), stop_words={'english'})),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=10,\n",
       "                               max_iter=2750, random_state=0,\n",
       "                               solver='lbfgs'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural = MLPClassifier(hidden_layer_sizes = 10, activation='tanh', solver='lbfgs', random_state=0, max_iter = 2750)\n",
    "tidf = toarray_Tidf(stop_words = {'english'}, ngram_range = (1,1))\n",
    "pipe_prog = make_pipeline(trans_Sentence(), tidf, neural)\n",
    "\n",
    "pipe_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max_features = [500,700,1100,None]\n",
    "num_alpha = [0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n",
      "trans (3551, 1)\n",
      "trans (3551, 1)\n",
      "trans (1523, 1)\n"
     ]
    }
   ],
   "source": [
    "train_errs = []\n",
    "val_errs = []\n",
    "best_val_err = float('inf')\n",
    "best_alpha = None\n",
    "best_best_feature = None\n",
    "for max_features in num_max_features:\n",
    "    for alpha in num_alpha:\n",
    "        pipe_prog.set_params(toarray_tidf__max_features= max_features, mlpclassifier__alpha=alpha)\n",
    "        let_fit = pipe_prog.fit(train_X_df,train_y_sr)\n",
    "        \n",
    "        error = (1- let_fit.score(train_X_df,train_y_sr))*100\n",
    "        train_errs.append(error)\n",
    "        \n",
    "        error = (1 - let_fit.score(val_X_df,val_y_sr))*100\n",
    "        val_errs.append(error)\n",
    "        \n",
    "        if (error < best_val_err):\n",
    "            best_val_err = error\n",
    "            best_alpha = alpha\n",
    "            best_feature = max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.101772816808934"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374    1\n",
       "2515    1\n",
       "2525    1\n",
       "2967    0\n",
       "4210    0\n",
       "       ..\n",
       "4232    0\n",
       "2041    1\n",
       "3205    0\n",
       "1389    1\n",
       "1323    1\n",
       "Name: Stt, Length: 3551, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_sr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
