{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STT: 34\n",
    "\n",
    "**Member**\n",
    "\n",
    "18120374 Nguyễn Minh Hiếu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thư viện cần dùng thêm:\n",
    "   * **ntlk**: để schemming\n",
    "   * **auttocorrect**: để kiểm tra chính tả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lợi ích:\n",
    "* Nếu tìm được công thức thì công ty sản xuất có thể tự động hóa quá trình rút trích ý kiến thích / không thích từ các đoạn đánh giá của người dùng\n",
    "* Giúp công ty đánh giá được hiệu quả công việc. (Ở đây là AMAZON có thể kiểm tra sản phẩm được vận chuyển có đúng ý muốn khách hàng hay không)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Thu thập dữ liệu\n",
    "Đưa một link sản phẩm bất kì và kiểm tra phần đánh giá của khách hàng.\n",
    "* Đánh giá sẽ phân loại:*postive* và *critical*\n",
    "* Mỗi trang gồm 10 đánh giá, ta duyệt 250 trang cho mỗi loại đánh giá. Vậy tổng cộng 10x250x2 = 5000 đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com/YI-Wireless-Security-Surveillance-Monitor/dp/B01CW4CEMS/ref=sr_1_47?dchild=1&qid=1610065358&s=electronics&sr=1-47'\n",
    "browser = webdriver.Edge(executable_path='./msedgedriver.exe') \n",
    "browser.maximize_window()\n",
    "browser.get(url)\n",
    "\n",
    "review_page = browser.find_element_by_link_text('See all reviews').click();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_types = browser.find_elements_by_css_selector('#reviewer-type-dropdown option')\n",
    "for reviewer_type in reviewer_types:\n",
    "    if \"Verified purchase only\" in reviewer_type.text:\n",
    "        reviewer_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_types = browser.find_elements_by_css_selector('#star-count-dropdown option')\n",
    "for comment_type in comment_types:\n",
    "    if \"All positive\" in comment_type.text:\n",
    "        comment_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = []\n",
    "status = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,260):\n",
    "    time.sleep(3)\n",
    "    html_text = browser.page_source\n",
    "    tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    reviews = tree.find_all('div',{'data-hook':'review'})\n",
    "    for review in reviews:\n",
    "        content = review.find('a', {'data-hook': 'review-title'})\n",
    "        phrases.append(content.text.strip())\n",
    "        status.append(1)\n",
    "    browser.find_element_by_class_name('a-last').click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_types = browser.find_elements_by_css_selector('#star-count-dropdown option')\n",
    "for comment_type in comment_types:\n",
    "    if \"All critical\" in comment_type.text:\n",
    "        comment_type.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,260):\n",
    "    time.sleep(3)\n",
    "    html_text = browser.page_source\n",
    "    tree = BeautifulSoup(html_text, 'html.parser')\n",
    "    reviews = tree.find_all('div',{'data-hook':'review'})\n",
    "    for review in reviews:\n",
    "        content = review.find('a', {'data-hook': 'review-title'})\n",
    "        phrases.append(content.text.strip())\n",
    "        status.append(0)\n",
    "    browser.find_element_by_class_name('a-last').click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Data = pd.DataFrame(\n",
    "    {'Para': phrases,\n",
    "     'Stt': status\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kiểm tra data**\n",
    "\n",
    "Lưu data vào file \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.to_csv('data.csv',index = False,encoding='utf-16',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cột Para sẽ chứa các đánh giá\n",
    "* cột status gồm loại đánh giá tương ứng (1: positive - 0: critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Para</th>\n",
       "      <th>Stt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>Great camera</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Para  Stt\n",
       "2458  Great camera    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv',encoding='utf-16',sep = '\\t')\n",
    "#data.head()\n",
    "data[data.index == 2458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5098, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dữ liệu trùng lặp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loại bỏ trùng lặp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4448, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra thiếu dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Para    0\n",
       "Stt     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra độ cân bằng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49280575539568344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Stt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sr = data[\"Stt\"] # sr là viết tắt của series\n",
    "X_df = data.drop(\"Stt\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df, val_X_df, train_y_sr, val_y_sr = train_test_split(X_df, y_sr, test_size=0.3, \n",
    "                                                              stratify=y_sr, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_sr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 2 tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cần thực hiện các bước: loại phần thừa, áp dụng *stemming* \n",
    "* Stemming: đưa các từ có hình thức tương đồng nhau về từ gốc (Ở đây ta chưa bàn về ngữ cảnh).\n",
    "* Phần thừa như dấu câu, icon.\n",
    "\n",
    "Vì ta sử dụng tiếng Anh nên các từ có dấu (khác a-z A-Z) cũng sẽ bị loại bỏ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "#nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import  StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recontruct_Sentence(sentence):\n",
    "    sentence = re.sub('[^A-Za-z]',' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(englishStemmer.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pipeline\n",
    "class trans_Sentence(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X_df, y=None):\n",
    "        return self\n",
    "    def transform(self, X_df, y=None):\n",
    "        # YOUR CODE HERE\n",
    "        optimize= []\n",
    "        self = X_df.copy()\n",
    "        sentence_s = self.Para\n",
    "        for sentence in sentence_s:\n",
    "            optimize.append(Recontruct_Sentence(sentence))\n",
    "        self.drop(['Para'],axis = 1)\n",
    "        self['Para'] = optimize\n",
    "        print('trans', self.shape)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toarray_Tidf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,max_features = None, ngram_range = None, stop_words = None):\n",
    "        self.max_features = max_features\n",
    "        self.ngram_range = ngram_range\n",
    "        self.stop_words = stop_words \n",
    "        vect = TfidfVectorizer(max_features =  max_features, ngram_range =  ngram_range, stop_words =  self.stop_words)\n",
    "        self.vect = vect\n",
    "    def fit(self, X_df, y=None):\n",
    "        self.vect.fit(X_df.Para)\n",
    "        return self\n",
    "    def transform(self, X_df, y=None):\n",
    "        # YOUR CODE HERE\n",
    "        return self.vect.transform(X_df.Para).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans_sentence', trans_Sentence()),\n",
       "                ('toarray_tidf',\n",
       "                 toarray_Tidf(ngram_range=(1, 1), stop_words={'english'})),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=10,\n",
       "                               max_iter=2750, random_state=0,\n",
       "                               solver='lbfgs'))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural = MLPClassifier(hidden_layer_sizes = 10, activation='tanh', solver='lbfgs', random_state=0, max_iter = 2750)\n",
    "tidf = toarray_Tidf(stop_words = {'english'}, ngram_range = (1,1))\n",
    "pipe_prog = make_pipeline(trans_Sentence(), tidf, neural)\n",
    "\n",
    "num_max_features = [500,700,1100,None]\n",
    "num_alpha = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n",
      "trans (3113, 1)\n",
      "trans (3113, 1)\n",
      "trans (1335, 1)\n"
     ]
    }
   ],
   "source": [
    "train_errs = []\n",
    "val_errs = []\n",
    "best_val_err = float('inf')\n",
    "best_alpha = None\n",
    "best_best_feature = None\n",
    "for max_features in num_max_features:\n",
    "    for alpha in num_alpha:\n",
    "        pipe_prog.set_params(toarray_tidf__max_features= max_features, mlpclassifier__alpha=alpha)\n",
    "        let_fit = pipe_prog.fit(train_X_df,train_y_sr)\n",
    "        \n",
    "        error = (1- let_fit.score(train_X_df,train_y_sr))*100\n",
    "        train_errs.append(error)\n",
    "        \n",
    "        error = (1 - let_fit.score(val_X_df,val_y_sr))*100\n",
    "        val_errs.append(error)\n",
    "        \n",
    "        if (error < best_val_err):\n",
    "            best_val_err = error\n",
    "            best_alpha = alpha\n",
    "            best_feature = max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.25468164794007"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_y_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = GridSearchCV(pipe_prog,pipe_para,cv=5,n_jobs=-1)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.fit(train_X_df, train_y_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-result.score(train_X_df, train_y_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1-result.score(val_X_df,val_y_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans (3113, 1)\n",
      "trans (1335, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = trans_Sentence()\n",
    "a = op.fit_transform(train_X_df)\n",
    "vect =  toarray_Tidf(stop_words = {'english'}, ngram_range = (1,1), max_features = 700)\n",
    "b = vect.fit(a)\n",
    "c =op.fit_transform(val_X_df)\n",
    "\n",
    "len(b.transform(a).toarray()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo mô hình Neural Net với: \n",
    "# * 1 tầng ẩn gồm 100 nơ-ron, hàm kích hoạt: tanh\n",
    "# * Thuật toán cực tiểu hóa: LBFGS, số vòng lặp tối đa: 1000\n",
    "# * random_state: 0\n",
    "neural_net_model2 = MLPClassifier(hidden_layer_sizes=(500,), activation='tanh',\n",
    "                                 solver='lbfgs', max_iter=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net_model2.fit(b, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - neural_net_model2.score(bag_of_word, train_y_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = trans_Sentence()\n",
    "a = op.fit_transform(test_X_df)\n",
    "vect =  TfidfVectorizer()\n",
    "b = vect.fit_transform(a)\n",
    "b.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - neural_net_model2.score(bag_of_word, test_y_sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
